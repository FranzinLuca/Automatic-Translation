# Automatic-Translation

This project is dedicated to translating non-modern Italian into modern Italian using large language models (LLMs).

Group name: Gradient Explorers

Luca Franzin 1886634            Francesco Casacchia 1698281

## Project Overview

The primary goal of this repository is to leverage and evaluate the capabilities of various LLMs for the nuanced task of translating historical Italian texts into their contemporary equivalents. The project involves a multi-stage process, from data preparation and model-driven translation to a comprehensive evaluation of the outputs.

The entire workflow, including data processing, translation, and evaluation, is detailed in the `translation.ipynb` Jupyter notebook.

## Repository Structure

```
.
├── .gitignore
├── README.md
├── dataset
│   ├── dataset_cleaned.csv
│   ├── dataset_deepseek.csv
│   ├── dataset_evaluated_deepseek.csv
│   ├── dataset_evaluated_mistral.csv
│   ├── dataset_evaluated_qwen.csv
│   ├── dataset_goldenLabel.csv
│   ├── dataset_mistral.csv
│   ├── dataset_qwen.csv
│   └── dataset_sub_20_m_eval.csv
├── Gradient_Explorers-hw2_transl-qwen.jsonl
├── Gradient_Explorers-hw2_transl-mistral.jsonl
├── Gradient_Explorers-hw2_transl-judge.jsonl
├── Gradient_Explorers-hw2_transl-deepseek.jsonl
├── Gradient_Explorers-hw2_manual_transl-20_sentences.jsonl
└── translation.ipynb
```

  - **`translation.ipynb`**: A Jupyter notebook that contains all the code for translating the dataset with the different models and evaluating the results.
  - **`dataset/`**: This directory houses all the data used in the project.
      - `dataset_cleaned.csv`: The initial dataset of sentences in non-modern Italian.
      - `dataset_goldenLabel.csv`: The ground truth, containing the modern Italian translations.
      - `dataset_deepseek.csv`, `dataset_mistral.csv`, `dataset_qwen.csv`: The translations of the dataset generated by the Deepseek, Mistral, and Qwen models.
      - `dataset_evaluated_deepseek.csv`, `dataset_evaluated_mistral.csv`, `dataset_evaluated_qwen.csv`: The datasets after being evaluated by the Prometheus model, including the assigned scores.
      - `dataset_sub_20_m_eval.csv`: A subset of 20 sentences with manual evaluations for the three models.

As requested, some CSV files are also available in JSONL format.

## Methodology

The project follows these steps, as detailed in `translation.ipynb`:

1.  **Translation**: Three different LLMs are used to translate the archaic Italian sentences from `dataset_cleaned.csv`. The models used are:

      * DeepSeek-R1-0528-Qwen3-8B
      * Mistral-7B-Instruct-v0.2
      * Qwen3-32B

2.  **Evaluation**: The performance of the translation models is assessed using two methods:

      * **Automated Evaluation**: The `prometheus-eval/prometheus-7b-v2.0` model is employed to grade the translations on a scale of 1 to 5, based on accuracy, fluency, and preservation of meaning.
      * **Manual Evaluation**: A subset of 20 translated sentences is manually graded by a human to serve as a benchmark for the automated evaluation.

3.  **Analysis**: The results from the automated and manual evaluations are compared using the following metrics:

      * **Cohen's Kappa**: To measure the agreement between the manual and automated scores.
      * **Spearman's Correlation**: To assess the monotonic relationship between the manual and automated scores.
      * **Mean Squared Error (MSE)**: To quantify the difference between the manual and automated scores.

## Results

The evaluation metrics comparing the manual and automated (Prometheus) scores for each model are as follows:

| Model | Cohen's Kappa | Spearman's Correlation | Mean Squared Error |
| :--- | :--- | :--- | :--- |
| **Mistral** | 0.0058 | 0.2134 | 2.5500 |
| **Deepseek** | 0.2691 | 0.1781 | 1.5500 |
| **Qwen** | -0.0332 | -0.1307 | 1.6000 |

Based on these results, the Deepseek model shows the highest agreement with manual evaluations, as indicated by its Cohen's Kappa score, and has the lowest MSE.