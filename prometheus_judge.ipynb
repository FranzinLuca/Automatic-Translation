{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088a3a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-12 17:54:40 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 06-12 17:54:41 [_custom_ops.py:22] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "from prometheus_eval.vllm import VLLM\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a66722a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Device string must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprometheus-eval/prometheus-7b-v2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m judge = PrometheusEval(model=model, absolute_grade_template=ABSOLUTE_PROMPT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/prometheus_eval/vllm.py:28\u001b[39m, in \u001b[36mVLLM.__init__\u001b[39m\u001b[34m(self, model, **vllm_kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     23\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     24\u001b[39m     **vllm_kwargs,\n\u001b[32m     25\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.model: \u001b[38;5;28mstr\u001b[39m = model\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28mself\u001b[39m.model: LLM = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m engine_args = EngineArgs(\n\u001b[32m    214\u001b[39m     model=model,\n\u001b[32m    215\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:494\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m engine_cls = \u001b[38;5;28mcls\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_USE_V1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1017\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m   1015\u001b[39m current_platform.pre_register_and_update()\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m device_config = \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m model_config = \u001b[38;5;28mself\u001b[39m.create_model_config()\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1 is unset, we enable V1 for \"supported features\"\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m#   and fall back to V0 for experimental or unsupported features.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1=1, we enable V1 for supported + experimental\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m#   features and raise error for unsupported features.\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1=0, we disable V1.\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/vllm/config.py:2337\u001b[39m, in \u001b[36mDeviceConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2334\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2335\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2336\u001b[39m     \u001b[38;5;66;03m# Set device with device type\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2337\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Device string must not be empty"
     ]
    }
   ],
   "source": [
    "model = VLLM(model=\"prometheus-eval/prometheus-7b-v2.0\")\n",
    "judge = PrometheusEval(model=model, absolute_grade_template=ABSOLUTE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e554ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "litellm.AuthenticationError: HuggingfaceException - Failed to fetch provider mapping: Client error '401 Unauthorized' for url 'https://huggingface.co/api/models/prometheus-eval/prometheus-7b?expand=inferenceProviderMapping'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/llms/huggingface/common_utils.py:84\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     83\u001b[39m response = httpx.get(path, headers=headers, params=params)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m provider_mapping = response.json().get(\u001b[33m\"\u001b[39m\u001b[33minferenceProviderMapping\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '401 Unauthorized' for url 'https://huggingface.co/api/models/prometheus-eval/prometheus-7b?expand=inferenceProviderMapping'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHuggingFaceError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/main.py:2254\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   2253\u001b[39m     hf_headers = headers \u001b[38;5;129;01mor\u001b[39;00m litellm.headers\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhuggingface_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33moobabooga\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:315\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[39m\n\u001b[32m    306\u001b[39m api_base = provider_config.get_complete_url(\n\u001b[32m    307\u001b[39m     api_base=api_base,\n\u001b[32m    308\u001b[39m     api_key=api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     litellm_params=litellm_params,\n\u001b[32m    313\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m data = \u001b[43mprovider_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/llms/huggingface/chat/transformation.py:131\u001b[39m, in \u001b[36mHuggingFaceChatConfig.transform_request\u001b[39m\u001b[34m(self, model, messages, optional_params, litellm_params, headers)\u001b[39m\n\u001b[32m    130\u001b[39m     model_id = model\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m provider_mapping = \u001b[43m_fetch_inference_provider_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m provider_mapping:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/llms/huggingface/common_utils.py:98\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     97\u001b[39m     headers = {}\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceError(\n\u001b[32m     99\u001b[39m     message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to fetch provider mapping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m     status_code=status_code,\n\u001b[32m    101\u001b[39m     headers=headers,\n\u001b[32m    102\u001b[39m )\n",
      "\u001b[31mHuggingFaceError\u001b[39m: Failed to fetch provider mapping: Client error '401 Unauthorized' for url 'https://huggingface.co/api/models/prometheus-eval/prometheus-7b?expand=inferenceProviderMapping'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m rubric_data = {\n\u001b[32m      6\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mcriteria\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mIs the model proficient in traqnslating archaic Italian sentences into modern Italian?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mscore1_description\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mThe model isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt capable of translating archaic Italian sentences into modern Italian, producing responses that are either completely incorrect or irrelevant.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mscore5_description\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mThe model excels in translating archaic Italian sentences into modern Italian, demonstrating a deep understanding of both the source and target languages, and consistently producing translations that are not only accurate but also stylistically appropriate.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m feedback, score = \u001b[43mjudge\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_absolute_grade\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrubric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_rubric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_answer\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFeedback:\u001b[39m\u001b[33m\"\u001b[39m, feedback)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mScore:\u001b[39m\u001b[33m\"\u001b[39m, score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/prometheus_eval/judge.py:81\u001b[39m, in \u001b[36mPrometheusEval.single_absolute_grade\u001b[39m\u001b[34m(self, instruction, response, rubric, reference_answer, params)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingle_absolute_grade\u001b[39m(\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     65\u001b[39m     instruction: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {},\n\u001b[32m     70\u001b[39m ) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    Grades a single response absolutely based on the provided instruction and response.\u001b[39;00m\n\u001b[32m     73\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \u001b[33;03m    :return: A tuple containing the feedback and score.\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     feedbacks, scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mabsolute_grade\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrubric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrubric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference_answers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreference_answer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreference_answer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m feedbacks[\u001b[32m0\u001b[39m], scores[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/prometheus_eval/judge.py:209\u001b[39m, in \u001b[36mPrometheusEval.absolute_grade\u001b[39m\u001b[34m(self, instructions, responses, rubric, reference_answers, params)\u001b[39m\n\u001b[32m    200\u001b[39m     feedbacks, scores = asyncio.run(\n\u001b[32m    201\u001b[39m         async_batch_completions_with_retries(\n\u001b[32m    202\u001b[39m             \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m         )\n\u001b[32m    207\u001b[39m     )\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     feedbacks, scores = \u001b[43mbatch_completions_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabsolute\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feedbacks, scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/prometheus_eval/utils.py:29\u001b[39m, in \u001b[36mbatch_completions_with_retries\u001b[39m\u001b[34m(model, inputs, mode, max_retries, params)\u001b[39m\n\u001b[32m     19\u001b[39m     params = {\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1024\u001b[39m,\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrepetition_penalty\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1.03\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.9\u001b[39m,\n\u001b[32m     25\u001b[39m     }\n\u001b[32m     27\u001b[39m total_len = \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m batched_outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m to_retry_inputs = []\n\u001b[32m     32\u001b[39m to_retry_indices = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/prometheus_eval/litellm.py:41\u001b[39m, in \u001b[36mLiteLLM.completions\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m kwargs = {\n\u001b[32m     35\u001b[39m     k: v\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrepetition_penalty\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muse_tqdm\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     38\u001b[39m }\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m tqdm(messages):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     result_responses.append(response.choices[\u001b[32m0\u001b[39m].message.content.strip())\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result_responses\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/utils.py:1285\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1282\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1283\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1284\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/utils.py:1163\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1161\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1162\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1164\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/main.py:3273\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3271\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3272\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3276\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2270\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2273\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Automatic-Translation/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1530\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_exception.status_code == \u001b[32m401\u001b[39m:\n\u001b[32m   1529\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\n\u001b[32m   1531\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHuggingfaceException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_exception.message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1532\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33mhuggingface\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1533\u001b[39m         model=model,\n\u001b[32m   1534\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1535\u001b[39m     )\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m   1537\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mAuthenticationError\u001b[39m: litellm.AuthenticationError: HuggingfaceException - Failed to fetch provider mapping: Client error '401 Unauthorized' for url 'https://huggingface.co/api/models/prometheus-eval/prometheus-7b?expand=inferenceProviderMapping'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401"
     ]
    }
   ],
   "source": [
    "instruction = \"The user needs to translate sentences from arcaic italian to modern italian. He provide this sentence to translate: crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi\",\n",
    "response = \"Crudele, e di tutte le colpe prendi vendetta, come dice la legge, e a nessun cavaliere perdona che pecchi.\"\n",
    "reference_answer = \"Crudele, e punisci tutte le colpe, come dice la legge, e non perdonare a nessun cavaliere che commetta peccato.\",\n",
    "\n",
    "rubric_data = {\n",
    "  \"criteria\":\"Is the model proficient in traqnslating archaic Italian sentences into modern Italian?\",\n",
    "  \"score1_description\":\"The model isn't capable of translating archaic Italian sentences into modern Italian, producing responses that are either completely incorrect or irrelevant.\",\n",
    "  \"score2_description\":\"The model is capable of translating some archaic Italian sentences into modern Italian, but the translations are often inaccurate or incomplete, failing to capture the essence of the original text.\",\n",
    "  \"score3_description\":\"The model typically translates archaic Italian sentences into modern Italian with moderate accuracy, but may struggle with complex phrases or idiomatic expressions, leading to occasional inaccuracies.\",\n",
    "  \"score4_description\":\"The model consistently translates archaic Italian sentences into modern Italian with high accuracy, capturing the essence of the original text and handling most idiomatic expressions effectively.\",\n",
    "  \"score5_description\":\"The model excels in translating archaic Italian sentences into modern Italian, demonstrating a deep understanding of both the source and target languages, and consistently producing translations that are not only accurate but also stylistically appropriate.\"\n",
    "}\n",
    "\n",
    "score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)\n",
    "\n",
    "\n",
    "feedback, score = judge.single_absolute_grade(\n",
    "    instruction=instruction,\n",
    "    response=response,\n",
    "    rubric=score_rubric,\n",
    "    reference_answer=reference_answer\n",
    ")\n",
    "\n",
    "print(\"Feedback:\", feedback)\n",
    "print(\"Score:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
