{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_eval.vllm import VLLM\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e554ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"The user needs to translate sentences from arcaic italian to modern italian. He provide this sentence to translate: crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi\",\n",
    "response = \"Crudele, e di tutte le colpe prendi vendetta, come dice la legge, e a nessun cavaliere perdona che pecchi.\"\n",
    "reference_answer = \"Crudele, e punisci tutte le colpe, come dice la legge, e non perdonare a nessun cavaliere che commetta peccato.\",\n",
    "\n",
    "rubric_data = {\n",
    "  \"criteria\":\"Is the model proficient in traqnslating archaic Italian sentences into modern Italian?\",\n",
    "  \"score1_description\":\"The model isn't capable of translating archaic Italian sentences into modern Italian, producing responses that are either completely incorrect or irrelevant.\",\n",
    "  \"score2_description\":\"The model is capable of translating some archaic Italian sentences into modern Italian, but the translations are often inaccurate or incomplete, failing to capture the essence of the original text.\",\n",
    "  \"score3_description\":\"The model typically translates archaic Italian sentences into modern Italian with moderate accuracy, but may struggle with complex phrases or idiomatic expressions, leading to occasional inaccuracies.\",\n",
    "  \"score4_description\":\"The model consistently translates archaic Italian sentences into modern Italian with high accuracy, capturing the essence of the original text and handling most idiomatic expressions effectively.\",\n",
    "  \"score5_description\":\"The model excels in translating archaic Italian sentences into modern Italian, demonstrating a deep understanding of both the source and target languages, and consistently producing translations that are not only accurate but also stylistically appropriate.\"\n",
    "}\n",
    "\n",
    "score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"prometheus-eval/prometheus-7b-v2.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prometheus-eval/prometheus-7b-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABS_SYSTEM_PROMPT = \"You are a helpful assistant that grades the quality of responses to user instructions. You will be given an instruction, a response, and a rubric. Your task is to provide feedback on the response and assign a score based on the rubric.\"\n",
    "ABSOLUTE_PROMPT = f\"Instruction: {{instruction}}\\n\\nResponse: {{response}}\\n\\nRubric: {{rubric}}\\n\\nReference Answer: {{reference_answer}}\\n\\nFeedback:\"\n",
    "user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT.format(\n",
    "    instruction=instruction,\n",
    "    response=response,\n",
    "    rubric=score_rubric,\n",
    "    reference_answer=reference_answer[0]\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
