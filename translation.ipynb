{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b5cf26",
   "metadata": {},
   "source": [
    "## Deepseek R1 qwen3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fd436",
   "metadata": {},
   "source": [
    "the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit\"\n",
    "device = \"cuda\" \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d45ea",
   "metadata": {},
   "source": [
    "an example of one translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7efd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Marco Cornelio ch'era de' dieci compagni, studiosamente  si riservò di parlare all'ultimo.\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"you are a translator from old italian to model italian. you take a sentence in old italian and you answer only with: La traduzione è:<translation> . Don't add anything else. Translate only in italian\"},\n",
    "        {\"role\": \"system\", \"content\": \"only use italian and no other language in the translation\"},\n",
    "        {\"role\": \"user\", \"content\": prompt} \n",
    "    ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=True, \n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "\n",
    "new_tokens = generated_ids[0][len(model_inputs[0]):]\n",
    "decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "decoded = decoded.split(\"La traduzione è:\")[-1].strip()\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4603e08",
   "metadata": {},
   "source": [
    "the code to translate all the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('dataset/dataset_cleaned.csv')\n",
    "\n",
    "# Create new column for translations\n",
    "df['Deepseek R1 qwen-8b'] = ''\n",
    "\n",
    "# Process each row\n",
    "for idx, row in df.iterrows():\n",
    "    # Create messages with current prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"you are a translator from old italian to model italian. you take a sentence in old italian and you answer only with: La traduzione è:<translation> . Don't add anything else. Translate only in italian\"},\n",
    "        {\"role\": \"system\", \"content\": \"only use italian and no other language in the translation\"},\n",
    "        {\"role\": \"user\", \"content\": row['Sentence']} \n",
    "    ]\n",
    "    \n",
    "    # Prepare input\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate translation\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=10000,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode and clean up response\n",
    "    new_tokens = generated_ids[0][len(model_inputs[0]):]\n",
    "    decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    translation = decoded.split(\"La traduzione è:\")[-1].strip()    \n",
    "    print(translation)\n",
    "    \n",
    "    # Store translation\n",
    "    df.at[idx, 'Deepseek R1 qwen-8b'] = translation\n",
    "\n",
    "# Save updated dataframe to dataset_deepseek.csv\n",
    "df.to_csv('./dataset/dataset_deepseek.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6675dd",
   "metadata": {},
   "source": [
    "cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724449d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the vram memory\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7705f8",
   "metadata": {},
   "source": [
    "## Mistral Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200df22",
   "metadata": {},
   "source": [
    "the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" \n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a026674",
   "metadata": {},
   "source": [
    "one translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91108ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"la moltitudine de' quali tu ài potuto vedere e riguardare lo studio e poco dinanzi udire le voci, e lle cui mani e lance apena posso ritenere.\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Sei un traduttore dall'italiano antico all'italiano moderno. Traduci una frase in italiano moderno e rispondi solo con: La traduzione è:<traduzione>. Non aggiungere altro pena la morte. Usa solo l'italiano e nessun'altra lingua nella traduzione.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt} \n",
    "    ]\n",
    "\n",
    "tokens = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "attention_mask = torch.ones_like(tokens)\n",
    "\n",
    "# Move to device\n",
    "model_inputs1 = tokens.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Generate with attention mask\n",
    "generated_ids1 = model.generate(\n",
    "    model_inputs1, \n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=1000, \n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Decode only the new tokens (exclude the input)\n",
    "new_tokens = generated_ids1[0][len(tokens[0]):]\n",
    "decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd11146",
   "metadata": {},
   "source": [
    "code for all database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/dataset_cleaned.csv')\n",
    "df['mistral'] = ''\n",
    "\n",
    "# Process each row\n",
    "for idx, row in df.iterrows():\n",
    "    # Create messages with current prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Sei un traduttore dall'italiano antico all'italiano moderno. Traduci una frase in italiano moderno e rispondi solo con: La traduzione è:<traduzione>. Non aggiungere altro pena la morte. Usa solo l'italiano e nessun'altra lingua nella traduzione.\"},\n",
    "        {\"role\": \"user\", \"content\": row['Sentence']} \n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "    attention_mask = torch.ones_like(tokens)\n",
    "\n",
    "    # Move to device\n",
    "    model_inputs1 = tokens.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Generate with attention mask\n",
    "    generated_ids1 = model.generate(\n",
    "        model_inputs1, \n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1000, \n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    new_tokens = generated_ids1[0][len(tokens[0]):]\n",
    "    decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    translation = decoded.split(\"La traduzione è:\")[-1].strip()\n",
    "    #remove any part between ( and  )\n",
    "    translation = translation.split('(')[0].strip()\n",
    "    #remove anything after the \\n character\n",
    "    translation = translation.split('\\n')[0].strip() \n",
    "    print(translation)\n",
    "    \n",
    "    # Store translation\n",
    "    df.at[idx, 'Mistral 7b-instruction'] = translation\n",
    "\n",
    "# Save updated dataframe to dataset_deepseek.csv\n",
    "df.to_csv('./dataset/dataset_mistral.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e06c4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the vram memory\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817ecae",
   "metadata": {},
   "source": [
    "## Qwen3-32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('dataset/dataset_cleaned.csv')\n",
    "df['qwen'] = ''\n",
    "model_name = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Create messages with current prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Sei un traduttore dall'italiano antico all'italiano moderno. Traduci una frase in italiano moderno e rispondi solo con: La traduzione è:<traduzione>. Non aggiungere altro pena la morte. Usa solo l'italiano e nessun'altra lingua nella traduzione.\"},\n",
    "        {\"role\": \"user\", \"content\": row['Sentence']} \n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True \n",
    "    )\n",
    "\n",
    "    tokens = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate with attention mask\n",
    "    generated_ids = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=5000,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=20,\n",
    "    )\n",
    "    new_tokens = generated_ids[0][len(tokens[0]):]\n",
    "    decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    translation = decoded.split(\"La traduzione è:\")[-1].strip()\n",
    "    print(translation)\n",
    "    \n",
    "    # Store translation\n",
    "    df.at[idx, 'Qwen3-32B'] = translation\n",
    "\n",
    "\n",
    "df.to_csv('./dataset/dataset_qwen.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327b390",
   "metadata": {},
   "source": [
    "## Prometeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe31b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:59:07.323281Z",
     "iopub.status.busy": "2025-06-21T07:59:07.322549Z",
     "iopub.status.idle": "2025-06-21T07:59:18.463191Z",
     "shell.execute_reply": "2025-06-21T07:59:18.462629Z",
     "shell.execute_reply.started": "2025-06-21T07:59:07.323250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c390c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:59:21.510933Z",
     "iopub.status.busy": "2025-06-21T07:59:21.510518Z",
     "iopub.status.idle": "2025-06-21T08:00:34.945263Z",
     "shell.execute_reply": "2025-06-21T08:00:34.944325Z",
     "shell.execute_reply.started": "2025-06-21T07:59:21.510908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16,\n",
    "                                            offload_folder=\"offload_prometheus\", offload_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af559e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T08:29:27.432047Z",
     "iopub.status.busy": "2025-06-21T08:29:27.431597Z",
     "iopub.status.idle": "2025-06-21T08:29:27.516066Z",
     "shell.execute_reply": "2025-06-21T08:29:27.515231Z",
     "shell.execute_reply.started": "2025-06-21T08:29:27.432023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_model_mistral = pandas.read_csv('dataset/dataset_mistral.csv')\n",
    "dataset_model_deepseek = pandas.read_csv('dataset/dataset_deepseek.csv')\n",
    "dataset_model_qwen = pandas.read_csv('dataset/dataset_qwen.csv')\n",
    "dataset_golden = pandas.read_csv('dataset/dataset_goldenLabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88571b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T09:10:46.761840Z",
     "iopub.status.busy": "2025-06-21T09:10:46.761168Z",
     "iopub.status.idle": "2025-06-21T09:10:46.768001Z",
     "shell.execute_reply": "2025-06-21T09:10:46.767337Z",
     "shell.execute_reply.started": "2025-06-21T09:10:46.761817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_instruction(sentence_to_translate, response, golden_label):\n",
    "    instruction = f\"Translate the following archaic Italian sentence into modern Italian: {sentence_to_translate}\",\n",
    "    response = f\"{response}\"\n",
    "    reference_answer = f\"{golden_label}\",\n",
    "    \n",
    "    rubric_data = {\n",
    "      \"criteria\": \"Archaic to Modern Italian Translation Quality\",\n",
    "      \"score1_description\": \"The translation fails to convey the core meaning or introduces incorrect information. It's fundamentally broken.\",\n",
    "      \"score2_description\": \"The translation contains significant grammatical or lexical errors or introduce new words that leads to a distorted or inaccurate understanding of the original meaning. While it may seem plausible, it ultimately misrepresents the concepts or events of the source text.\",\n",
    "      \"score3_description\": \"The translation preserves the original meaning accurately, but its presentation is flawed. It is difficult to read due to unnatural phrasing, incorrect modern syntax, or other stylistic errors.\",\n",
    "      \"score4_description\": \"The translation is accurate, grammatically correct, and almost entirely fluent. It faithfully preserves the original meaning (subjects, concepts, and events are the same).\",\n",
    "      \"score5_description\": \"The translation is grammatically perfect, accurate, and reads as completely natural, fluent modern Italian. It effectively modernizes all archaic elements and skillfully captures the tone and nuances of the original text.\"\n",
    "    }\n",
    "    \n",
    "    score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)\n",
    "    \n",
    "    ABS_SYSTEM_PROMPT = \"You need to grade the quality of responses to a given instruction. You will be given an instruction, a response, and a rubric and a reference answer. Your task is to assign a score based on the rubric confronting the response to the reference given. Your should STRICTLY give ALWAYS the score in this style --> [Score: ].\"\n",
    "    ABSOLUTE_PROMPT = f\"Instruction: {{instruction}}\\n\\nResponse: {{response}}\\n\\nRubric: {{rubric}}\\n\\nReference Answer: {{reference_answer}}\\n\\nFeedback:\"\n",
    "    user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT.format(\n",
    "        instruction=instruction,\n",
    "        response=response,\n",
    "        rubric=score_rubric,\n",
    "        reference_answer=reference_answer[0]\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_score(text):\n",
    "    \"\"\"\n",
    "    Extract score from feedback text using regex\n",
    "    \"\"\"\n",
    "    pattern = r'\\[Score:\\s*(\\d+)\\]'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa44e8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T08:29:30.372035Z",
     "iopub.status.busy": "2025-06-21T08:29:30.371471Z",
     "iopub.status.idle": "2025-06-21T08:29:30.381998Z",
     "shell.execute_reply": "2025-06-21T08:29:30.381234Z",
     "shell.execute_reply.started": "2025-06-21T08:29:30.372016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "length = len(dataset_golden)\n",
    "list_of_messages_m = []\n",
    "list_of_messages_d = []\n",
    "list_of_messages_q = []\n",
    "for i in range(0, length):\n",
    "    sentence_to_translate = dataset_golden[\"Sentence\"][i]\n",
    "    response_mistral = dataset_model_mistral[\"Mistral 7b-instruction\"][i]\n",
    "    response_deepseek = dataset_model_deepseek[\"Deepseek R1 qwen-8b\"][i]\n",
    "    response_qwen = dataset_model_qwen[\"Qwen3-32b\"][i]\n",
    "    golden_label = dataset_golden[\"goldenLabel\"][i]\n",
    "    message_mistral = create_instruction(sentence_to_translate, response_mistral, golden_label)\n",
    "    message_deepseek = create_instruction(sentence_to_translate, response_deepseek, golden_label)\n",
    "    message_qwen = create_instruction(sentence_to_translate, response_qwen, golden_label)\n",
    "    list_of_messages_m.append(message_mistral)\n",
    "    list_of_messages_d.append(message_deepseek)\n",
    "    list_of_messages_q.append(message_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d8159",
   "metadata": {},
   "source": [
    "### Evaluating Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e08a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T08:09:32.370129Z",
     "iopub.status.busy": "2025-06-21T08:09:32.369468Z",
     "iopub.status.idle": "2025-06-21T08:27:00.604687Z",
     "shell.execute_reply": "2025-06-21T08:27:00.604081Z",
     "shell.execute_reply.started": "2025-06-21T08:09:32.370098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_outputs_m = []\n",
    "all_scores_m = []\n",
    "\n",
    "print(\"Starting evaluation for Mistral 7B-Instruct\")\n",
    "model.to(device)\n",
    "pbar = tqdm(list_of_messages_m, desc=\"Processing data\")\n",
    "\n",
    "for msg in pbar:\n",
    "    encodeds = tokenizer.apply_chat_template(msg, return_tensors=\"pt\", return_dict = True)\n",
    "    model_inputs = encodeds['input_ids'].to(device)\n",
    "    attention_mask = encodeds['attention_mask'].to(device)\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=500, attention_mask=attention_mask, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    all_outputs_m.append(decoded[0])\n",
    "    output = extract_score(decoded[0])\n",
    "    if output == None:\n",
    "        output = 1\n",
    "    all_scores_m.append(output)\n",
    "    if all_scores_m:\n",
    "        avg_score = sum(all_scores_m) / len(all_scores_m)\n",
    "        pbar.set_postfix(last_score=output, avg_score=f'{avg_score:.2f}')\n",
    "print(all_scores_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c0e9c",
   "metadata": {},
   "source": [
    "### Evaluating Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85072cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T08:29:36.640016Z",
     "iopub.status.busy": "2025-06-21T08:29:36.639741Z",
     "iopub.status.idle": "2025-06-21T08:47:36.099588Z",
     "shell.execute_reply": "2025-06-21T08:47:36.098905Z",
     "shell.execute_reply.started": "2025-06-21T08:29:36.639996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_outputs_d = []\n",
    "all_scores_d = []\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting evaluation for Deepseek R1 Qwen-8B\")\n",
    "pbar = tqdm(list_of_messages_d, desc=\"Processing data\")\n",
    "for msg in pbar:\n",
    "    encodeds = tokenizer.apply_chat_template(msg, return_tensors=\"pt\", return_dict = True)\n",
    "    model_inputs = encodeds['input_ids'].to(device)\n",
    "    attention_mask = encodeds['attention_mask'].to(device)\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=500, attention_mask=attention_mask, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    all_outputs_d.append(decoded[0])\n",
    "    output = extract_score(decoded[0])\n",
    "    if output == None:\n",
    "        output = 1\n",
    "    all_scores_d.append(output)\n",
    "    if all_scores_d:\n",
    "        avg_score = sum(all_scores_d) / len(all_scores_d)\n",
    "        pbar.set_postfix(last_score=output, avg_score=f'{avg_score:.2f}')\n",
    "print(all_scores_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74feb2c9-fd2a-4a89-94e6-fb9e32261e34",
   "metadata": {},
   "source": [
    "### Evaluating Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865218c9-66be-4cf9-9808-dbc64fc21985",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_outputs_q = []\n",
    "all_scores_q = []\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting evaluation for Qwen3\")\n",
    "pbar = tqdm(list_of_messages_q, desc=\"Processing data\")\n",
    "for msg in pbar:\n",
    "    encodeds = tokenizer.apply_chat_template(msg, return_tensors=\"pt\", return_dict = True)\n",
    "    model_inputs = encodeds['input_ids'].to(device)\n",
    "    attention_mask = encodeds['attention_mask'].to(device)\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=500, attention_mask=attention_mask, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    all_outputs_q.append(decoded[0])\n",
    "    output = extract_score(decoded[0])\n",
    "    if output == None:\n",
    "        output = 1\n",
    "    all_scores_q.append(output)\n",
    "    if all_scores_q:\n",
    "        avg_score = sum(all_scores_q) / len(all_scores_q)\n",
    "        pbar.set_postfix(last_score=output, avg_score=f'{avg_score:.2f}')\n",
    "print(all_scores_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3666c3",
   "metadata": {},
   "source": [
    "### Cleaning cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f9723e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.755846Z",
     "iopub.status.busy": "2025-06-20T18:35:26.755614Z",
     "iopub.status.idle": "2025-06-20T18:35:26.773688Z",
     "shell.execute_reply": "2025-06-20T18:35:26.773163Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.755830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "del tokenizer\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3af22",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c111c1e",
   "metadata": {},
   "source": [
    "#### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41801413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.775896Z",
     "iopub.status.busy": "2025-06-20T18:35:26.775699Z",
     "iopub.status.idle": "2025-06-20T18:35:26.805304Z",
     "shell.execute_reply": "2025-06-20T18:35:26.804627Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.775881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_evaluated_mistral = dataset_model_mistral.copy()\n",
    "dataset_evaluated_mistral['p_mistral_vote'] = all_scores_m\n",
    "dataset_evaluated_mistral['golden_label'] = dataset_golden[\"goldenLabel\"]\n",
    "dataset_evaluated_mistral.info()\n",
    "# Save the evaluated dataset\n",
    "dataset_evaluated_mistral.to_csv('./dataset_evaluated_mistral.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eec09f",
   "metadata": {},
   "source": [
    "#### Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaeffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.806119Z",
     "iopub.status.busy": "2025-06-20T18:35:26.805876Z",
     "iopub.status.idle": "2025-06-20T18:35:26.823484Z",
     "shell.execute_reply": "2025-06-20T18:35:26.822868Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.806104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_evaluated_deepseek = dataset_model_deepseek.copy()\n",
    "dataset_evaluated_deepseek['p_deepseek_vote'] = all_scores_d\n",
    "dataset_evaluated_deepseek['golden_label'] = dataset_golden[\"goldenLabel\"]\n",
    "dataset_evaluated_deepseek.info()\n",
    "# Save the evaluated dataset\n",
    "dataset_evaluated_deepseek.to_csv('./dataset_evaluated_deepseek.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c04ef7-8289-4480-9bcd-8555f4f96d30",
   "metadata": {},
   "source": [
    "#### Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d4d70-29ed-4fe7-a546-e14549178330",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_evaluated_qwen = dataset_model_qwen.copy()\n",
    "dataset_evaluated_qwen['p_qwen_vote'] = all_scores_q\n",
    "dataset_evaluated_qwen['golden_label'] = dataset_golden[\"goldenLabel\"]\n",
    "dataset_evaluated_qwen.info()\n",
    "# Save the evaluated dataset\n",
    "dataset_evaluated_qwen.to_csv('./dataset_evaluated_qwen.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a7408",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba2ffc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.824337Z",
     "iopub.status.busy": "2025-06-20T18:35:26.824144Z",
     "iopub.status.idle": "2025-06-20T18:35:26.858394Z",
     "shell.execute_reply": "2025-06-20T18:35:26.857809Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.824323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "manual = pd.read_csv('dataset/dataset_sub_20_m_eval.csv')\n",
    "mistral = pd.read_csv('dataset/dataset_evaluated_mistral.csv')\n",
    "deepseek = pd.read_csv('dataset/dataset_evaluated_deepseek.csv')\n",
    "qwen = pd.read_csv('dataset/dataset_evaluated_qwen.csv')\n",
    "\n",
    "# select only the same sentence present in both mistral/deepseek and manual\n",
    "mistral_sub = mistral[mistral['golden_label'].isin(manual['goldenLabel'])]\n",
    "deepseek_sub = deepseek[deepseek['golden_label'].isin(manual['goldenLabel'])]\n",
    "qwen_sub = qwen[qwen['golden_label'].isin(manual['goldenLabel'])]\n",
    "\n",
    "# order the mistral_sub, deepseek_sub and manual by golden_label\n",
    "mistral_sub = mistral_sub.sort_values(by='golden_label').reset_index(drop=True)\n",
    "deepseek_sub = deepseek_sub.sort_values(by='golden_label').reset_index(drop=True)\n",
    "qwen_sub = qwen_sub.sort_values(by='golden_label').reset_index(drop=True)\n",
    "manual = manual.sort_values(by='goldenLabel').reset_index(drop=True)\n",
    "\n",
    "# get the scores\n",
    "mistral_scores = mistral_sub['p_mistral_vote'].values\n",
    "deepseek_scores = deepseek_sub['p_deepseek_vote'].values\n",
    "qwen_scores = qwen_sub['p_qwen_vote'].values\n",
    "manual_scores_d = manual['deepseek_vote'].values\n",
    "manual_scores_m = manual['mistral_vote'].values\n",
    "manual_scores_q = manual['qwen_vote'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d824a5a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.859670Z",
     "iopub.status.busy": "2025-06-20T18:35:26.859184Z",
     "iopub.status.idle": "2025-06-20T18:35:26.880211Z",
     "shell.execute_reply": "2025-06-20T18:35:26.879530Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.859645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for Mistral: 0.0058\n",
      "Cohen's Kappa for Deepseek: 0.2691\n",
      "Cohen's Kappa for Qwen: -0.0332\n",
      "Spearman's correlation for Mistral: 0.2134\n",
      "Spearman's correlation for Deepseek: 0.1781\n",
      "Spearman's correlation for Qwen: -0.1307\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cohen's Kappa for Mistral\n",
    "kappa_mistral = cohen_kappa_score(manual_scores_m, mistral_scores)\n",
    "# Calculate Cohen's Kappa for Deepseek\n",
    "kappa_deepseek = cohen_kappa_score(manual_scores_d, deepseek_scores)\n",
    "# Calculate Cohen's Kappa for Qwen\n",
    "kappa_qwen = cohen_kappa_score(manual_scores_q, qwen_scores)\n",
    "\n",
    "# Calculate Spearman's correlation for Mistral\n",
    "spearman_mistral = spearmanr(manual_scores_m, mistral_scores).correlation\n",
    "# Calculate Spearman's correlation for Deepseek\n",
    "spearman_deepseek = spearmanr(manual_scores_d, deepseek_scores).correlation\n",
    "# Calculate Spearman's correlation for Qwen\n",
    "spearman_qwen = spearmanr(manual_scores_q, qwen_scores).correlation\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cohen's Kappa for Mistral: {kappa_mistral:.4f}\")\n",
    "print(f\"Cohen's Kappa for Deepseek: {kappa_deepseek:.4f}\")\n",
    "print(f\"Cohen's Kappa for Qwen: {kappa_qwen:.4f}\")\n",
    "print(f\"Spearman's correlation for Mistral: {spearman_mistral:.4f}\")\n",
    "print(f\"Spearman's correlation for Deepseek: {spearman_deepseek:.4f}\")\n",
    "print(f\"Spearman's correlation for Qwen: {spearman_qwen:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf27169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T18:35:26.881228Z",
     "iopub.status.busy": "2025-06-20T18:35:26.880982Z",
     "iopub.status.idle": "2025-06-20T18:35:26.886729Z",
     "shell.execute_reply": "2025-06-20T18:35:26.886057Z",
     "shell.execute_reply.started": "2025-06-20T18:35:26.881211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Mistral: 2.5500\n",
      "Mean Squared Error for Deepseek: 1.5500\n",
      "Mean Squared Error for Qwen: 1.6000\n"
     ]
    }
   ],
   "source": [
    "# calculate the MSE for mistral and deepseek\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_mistral = mean_squared_error(manual_scores_m, mistral_scores)\n",
    "mse_deepseek = mean_squared_error(manual_scores_d, deepseek_scores)\n",
    "mse_qwen = mean_squared_error(manual_scores_q, qwen_scores)\n",
    "print(f\"Mean Squared Error for Mistral: {mse_mistral:.4f}\")\n",
    "print(f\"Mean Squared Error for Deepseek: {mse_deepseek:.4f}\")\n",
    "print(f\"Mean Squared Error for Qwen: {mse_qwen:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d280cda",
   "metadata": {},
   "source": [
    "## Convert in jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db2a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import jsonlines\n",
    "\n",
    "with open('dataset/dataset_deepseek.csv', mode='r', newline='', encoding='utf-8') as csvfile, jsonlines.open('Gradient_Explorers-hw2_transl-deepseek.jsonl', mode='w') as writer:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        writer.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cd3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/dataset_mistral.csv', mode='r', newline='', encoding='utf-8') as csvfile, jsonlines.open('Gradient_Explorers-hw2_transl-mistral.jsonl', mode='w') as writer:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        writer.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3a8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/dataset_qwen.csv', mode='r', newline='', encoding='utf-8') as csvfile, jsonlines.open('Gradient_Explorers-hw2_transl-qwen.jsonl', mode='w') as writer:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        writer.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open 3 file, dataset_evaluated_mistral.csv, dataset_evaluated_deepseek.csv, dataset_evaluated_qwen.csv and unite them in a single jsonl file called Gradient_Explorers-hw2_transl-evaluated.jsonl\n",
    "with open('dataset/dataset_evaluated_mistral.csv', mode='r', newline='', encoding='utf-8') as csvfile_m, \\\n",
    "     open('dataset/dataset_evaluated_deepseek.csv', mode='r', newline='', encoding='utf-8') as csvfile_d, \\\n",
    "     open('dataset/dataset_evaluated_qwen.csv', mode='r', newline='', encoding='utf-8') as csvfile_q, \\\n",
    "     jsonlines.open('Gradient_Explorers-hw2_transl-judge.jsonl', mode='w') as writer:\n",
    "    reader_m = csv.DictReader(csvfile_m)\n",
    "    reader_d = csv.DictReader(csvfile_d)\n",
    "    reader_q = csv.DictReader(csvfile_q)\n",
    "    for row_m, row_d, row_q in zip(reader_m, reader_d, reader_q):\n",
    "        combined_row = {\n",
    "            'Sentence': row_m['Sentence'],\n",
    "            'goldenLabel': row_m['golden_label'],\n",
    "            'mistral_vote': row_m['p_mistral_vote'],\n",
    "            'deepseek_vote': row_d['p_deepseek_vote'],\n",
    "            'qwen_vote': row_q['p_qwen_vote']\n",
    "        }\n",
    "        writer.write(combined_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d627d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/dataset_sub_20_m_eval.csv', mode='r', newline='', encoding='utf-8') as csvfile, jsonlines.open('Gradient_Explorers-hw2_manual_transl-20_sentences.jsonl', mode='w') as writer:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        writer.write(row)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12780810,
     "datasetId": 7655949,
     "sourceId": 12235457,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12700163,
     "datasetId": 7655934,
     "sourceId": 12164255,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12701373,
     "datasetId": 7661970,
     "sourceId": 12165363,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
